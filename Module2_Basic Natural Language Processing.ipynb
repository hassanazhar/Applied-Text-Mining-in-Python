{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 (Python 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NLP Tasks with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# download the text corpora\n",
    "nltk.download()\n",
    "\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting vocabulary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1: Call me Ishmael .\n",
      "sent2: The family of Dashwood had long been settled in Sussex .\n",
      "sent3: In the beginning God created the heaven and the earth .\n",
      "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "sent5: I have a problem with people PMing me to lol JOIN\n",
      "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
      "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
      "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Call', 'Ishmael']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 # <Text: Moby Dick by Herman Melville 1851>\n",
    "sents() # look at all the sentences\n",
    "sent1 # ['Call', 'me', 'Ishmael', '.']\n",
    "\n",
    "len(sent1) # 4\n",
    "len(text1) # 260819\n",
    "\n",
    "set(text1) # look at unique words \n",
    "list(set(sent1))[:2] # look at the first 2 words in text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call', 'me', 'Ishmael']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = FreqDist(sent1) # frequency \n",
    "dist # FreqDist({'.': 1, 'Call': 1, 'Ishmael': 1, 'me': 1})\n",
    "\n",
    "vocab1 = dist.keys() # actual words\n",
    "vocab1 # dict_keys(['Call', 'me', 'Ishmael', '.'])\n",
    "list(vocab1)[:2] # ['Call', 'me']\n",
    "\n",
    "dist['me'] # how many times a particularly word occurs # 1\n",
    "\n",
    "freqwords = [w for w in vocab1 if len(w) > 1 and dist[w] >= 1] # how many times a particular word occurs and also have a condition on the length of the word\n",
    "freqwords # ['Call', 'me', 'Ishmael']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'list', 'list', 'list', 'list']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# different forms of the same'word'\n",
    "input1 = \"List listed lists listing listings\"\n",
    "\n",
    "words1 = input1.lower().split(' ') # normalization - lowercase\n",
    "words1 # ['list', 'listed', 'lists', 'listing', 'listings']\n",
    "\n",
    "porter = nltk.PorterStemmer() # # find the root/root form of the word\n",
    "[porter.stem(t) for t in words1] # ['list', 'list', 'list', 'list', 'list']\n",
    "# notes - you may want to keep 'listing' as different from 'list'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Universal', 'Delcaration', 'of', 'Human', 'Right']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when stemming returns invalid words\n",
    "text2 = 'Universal Delcaration of Human Right'\n",
    "text2 = text2.split(' ')\n",
    "[porter.stem(t) for t in text2] # ['univers', 'delcar', 'of', 'human', 'right']\n",
    "\n",
    "# lemmatization returns valid words\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "[WNlemma.lemmatize(t) for t in text2] # ['Universal', 'Delcaration', 'of', 'Human', 'Right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.',\n",
       " 'A gallon of milk in the U.S. costs $2.99.',\n",
       " 'Is this the third sentence?',\n",
       " 'Yes, it is!']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall splitting a sentence into words/tokens\n",
    "text11 = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "text11.split(' ') # ['Children', \"shouldn't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed.']\n",
    "\n",
    "# NLTK has an inbuilt tokenizer\n",
    "nltk.word_tokenize(text11) #['Children','should',\"n't\",'drink','a','sugary','drink','before','bed','.']\n",
    "# notes - you may want clear about 'should' 'n't' \n",
    "\n",
    "# Setence splitting\n",
    "text12 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
    "nltk.sent_tokenize(text12) \n",
    "# ['This is the first sentence.',\n",
    "# 'A gallon of milk in the U.S. costs $2.99.',\n",
    "# 'Is this the third sentence?',\n",
    "# 'Yes, it is!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Advanced NLP Tasks with NLTK\n",
    "\n",
    "# NLP Tasks \n",
    "# Counting words, counting frequency of words\n",
    "# Finding sentence boundaries\n",
    "# Part of speech tagging\n",
    "# parsing the sentence structure \n",
    "# Identifying semantic role labeling\n",
    "# Named entity recognition\n",
    "# Co-reference and pronoun resoluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n"
     ]
    }
   ],
   "source": [
    "# Tag  Wordclass    Tag  Wordclass   Tag  Wordclass\n",
    "# CC   Conjunction  JJ   Adjective   PRP  Pronoun\n",
    "# CD   Cardinal     MD   Modal       RB   Adverb\n",
    "# DT   Determiner   NN   Noun        SYM  Symbol \n",
    "# IN   Preposition  POS  Possessive  VB   Verb\n",
    "\n",
    "import nltk \n",
    "nltk.help.upenn_tagset('MD')\n",
    "# MD: modal auxiliary\n",
    "# can cannot could couldn't dare may might must need ought shall should\n",
    "# shouldn't will would"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Children', 'NNP'),\n",
       " ('should', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('drink', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('sugary', 'JJ'),\n",
       " ('drink', 'NN'),\n",
       " ('before', 'IN'),\n",
       " ('bed', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text13 = nltk.word_tokenize(text11)\n",
    "nltk.pos_tag(text13) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Visiting', 'VBG'),\n",
       " ('aunts', 'NNS'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('nuisance', 'NN')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ambiguity in POS Tagging \n",
    "text14 = nltk.word_tokenize(\"Visiting aunts can be a nuisance\")\n",
    "nltk.pos_tag(text14)\n",
    "# [('Visiting', 'VBG'),\n",
    "# ('aunts', 'NNS'),\n",
    "# ('can', 'MD'),\n",
    "# ('be', 'VB'),\n",
    "# ('a', 'DT'),\n",
    "# ('nuisance', 'NN')]\n",
    "\n",
    "# Another alternative POS tagging that arent show by pos_tag\n",
    "# [('Visiting', 'JJ'),\n",
    "# ('aunts', 'NNS'),\n",
    "# ('can', 'MD'),\n",
    "# ('be', 'VB'),\n",
    "# ('a', 'DT'),\n",
    "# ('nuisance', 'NN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Alice) (VP (V loves) (NP Bob)))\n"
     ]
    }
   ],
   "source": [
    "# Parsing sentence structure\n",
    "# Making sense of sentences is eady if they follow a well-defined grammatical structure\n",
    "\n",
    "text15 = nltk.word_tokenize(\"Alice loves Bob\")\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP  \n",
    "VP -> V NP \n",
    "NP -> 'Alice' | 'Bob'\n",
    "V -> 'loves'\n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "trees = parser.parse_all(text15)\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V saw) (NP (DT the) (N man)))\n",
      "    (PP (P with) (NP (DT a) (N telescope)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (DT the) (N man) (PP (P with) (NP (DT a) (N telescope))))))\n"
     ]
    }
   ],
   "source": [
    "# Ambiguity in Parsing\n",
    "# Ambiguity may exist even if sentences are grammaatically corret! \n",
    "\n",
    "text16 = nltk.word_tokenize(\"I saw the man with a telescope\")\n",
    "grammar1b = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP  \n",
    "VP -> V NP | VP PP\n",
    "PP -> P NP\n",
    "NP -> DT N | DT N PP | 'I'\n",
    "DT -> 'a' | 'the'\n",
    "N -> 'man' | 'telescope'\n",
    "V -> 'saw'\n",
    "P -> 'with' \n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar1b)\n",
    "trees = parser.parse_all(text16)\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "# NLTK and Parse Tree Collection \n",
    "from nltk.corpus import treebank\n",
    "text17 = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "print(text17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'), ('old', 'JJ'), ('man', 'NN'), ('the', 'DT'), ('boat', 'NN')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## POS tagging and parsing ambiguity\n",
    "# Uncommon usages of words\n",
    "text18 = nltk.word_tokenize(\"The old man the boat\")\n",
    "nltk.pos_tag(text18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Colorless', 'NNP'),\n",
       " ('green', 'JJ'),\n",
       " ('ideas', 'NNS'),\n",
       " ('sleep', 'VBP'),\n",
       " ('furiously', 'RB')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Well-formed sentences may sill be meaningless! \n",
    "text19 = nltk.word_tokenize(\"Colorless green ideas sleep furiously\")\n",
    "nltk.pos_tag(text19)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
